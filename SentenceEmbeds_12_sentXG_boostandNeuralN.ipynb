{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XG_boostandNeuralN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSWl22Qk0gms"
      },
      "source": [
        "Mounting the files on google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJn0CbfgCtD3",
        "outputId": "a2e02a43-a203-4895-a9bf-1eba33a8742c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJls-yTxCxdE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KxgQXI300Q7"
      },
      "source": [
        "Train data\n",
        "\n",
        "Removing the unnecessary features and keeping the relevant features.\n",
        "\n",
        "Converting Y label strings to numerical value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z87NBX9uDZtT"
      },
      "source": [
        "d = pd.read_csv('/content/drive/MyDrive/EE769Project/glove_sent_encoding_data_12Sent.csv')\n",
        "d.drop([\"Unnamed: 0\",\"Code_blocks\",\"Num_Tags\",\"Body_size\",\"Extra_tags\",\"Code_Lines\",\"Num Tags\"], axis=1,inplace=True)\n",
        "# To convert string to numeric value\n",
        "new_values = {'HQ':2, 'LQ_EDIT':1, 'LQ_CLOSE':0}\n",
        "d['Y']  = d.Y.replace(new_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zewFgVT206Lz"
      },
      "source": [
        "Display the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "KV13CZCYCFiD",
        "outputId": "893cc563-95c3-40a0-e36e-b2b3a6d665d9"
      },
      "source": [
        "d.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>Body_1</th>\n",
              "      <th>Body_2</th>\n",
              "      <th>Body_3</th>\n",
              "      <th>Body_4</th>\n",
              "      <th>Body_5</th>\n",
              "      <th>Body_6</th>\n",
              "      <th>Body_7</th>\n",
              "      <th>Body_8</th>\n",
              "      <th>Body_9</th>\n",
              "      <th>Body_10</th>\n",
              "      <th>Body_11</th>\n",
              "      <th>Body_12</th>\n",
              "      <th>Body_13</th>\n",
              "      <th>Body_14</th>\n",
              "      <th>Body_15</th>\n",
              "      <th>Body_16</th>\n",
              "      <th>Body_17</th>\n",
              "      <th>Body_18</th>\n",
              "      <th>Body_19</th>\n",
              "      <th>Body_20</th>\n",
              "      <th>Body_21</th>\n",
              "      <th>Body_22</th>\n",
              "      <th>Body_23</th>\n",
              "      <th>Body_24</th>\n",
              "      <th>Body_25</th>\n",
              "      <th>Body_26</th>\n",
              "      <th>Body_27</th>\n",
              "      <th>Body_28</th>\n",
              "      <th>Body_29</th>\n",
              "      <th>Body_30</th>\n",
              "      <th>Body_31</th>\n",
              "      <th>Body_32</th>\n",
              "      <th>Body_33</th>\n",
              "      <th>Body_34</th>\n",
              "      <th>Body_35</th>\n",
              "      <th>Body_36</th>\n",
              "      <th>Body_37</th>\n",
              "      <th>Body_38</th>\n",
              "      <th>Body_39</th>\n",
              "      <th>...</th>\n",
              "      <th>Title_261</th>\n",
              "      <th>Title_262</th>\n",
              "      <th>Title_263</th>\n",
              "      <th>Title_264</th>\n",
              "      <th>Title_265</th>\n",
              "      <th>Title_266</th>\n",
              "      <th>Title_267</th>\n",
              "      <th>Title_268</th>\n",
              "      <th>Title_269</th>\n",
              "      <th>Title_270</th>\n",
              "      <th>Title_271</th>\n",
              "      <th>Title_272</th>\n",
              "      <th>Title_273</th>\n",
              "      <th>Title_274</th>\n",
              "      <th>Title_275</th>\n",
              "      <th>Title_276</th>\n",
              "      <th>Title_277</th>\n",
              "      <th>Title_278</th>\n",
              "      <th>Title_279</th>\n",
              "      <th>Title_280</th>\n",
              "      <th>Title_281</th>\n",
              "      <th>Title_282</th>\n",
              "      <th>Title_283</th>\n",
              "      <th>Title_284</th>\n",
              "      <th>Title_285</th>\n",
              "      <th>Title_286</th>\n",
              "      <th>Title_287</th>\n",
              "      <th>Title_288</th>\n",
              "      <th>Title_289</th>\n",
              "      <th>Title_290</th>\n",
              "      <th>Title_291</th>\n",
              "      <th>Title_292</th>\n",
              "      <th>Title_293</th>\n",
              "      <th>Title_294</th>\n",
              "      <th>Title_295</th>\n",
              "      <th>Title_296</th>\n",
              "      <th>Title_297</th>\n",
              "      <th>Title_298</th>\n",
              "      <th>Title_299</th>\n",
              "      <th>Title_300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.240987</td>\n",
              "      <td>0.294838</td>\n",
              "      <td>0.043283</td>\n",
              "      <td>-0.216777</td>\n",
              "      <td>-0.273428</td>\n",
              "      <td>-0.262819</td>\n",
              "      <td>-0.128607</td>\n",
              "      <td>-0.047490</td>\n",
              "      <td>0.206232</td>\n",
              "      <td>-1.043640</td>\n",
              "      <td>0.093642</td>\n",
              "      <td>-0.065293</td>\n",
              "      <td>-0.091981</td>\n",
              "      <td>0.128572</td>\n",
              "      <td>0.199107</td>\n",
              "      <td>-0.104559</td>\n",
              "      <td>-0.101942</td>\n",
              "      <td>0.023553</td>\n",
              "      <td>-0.073419</td>\n",
              "      <td>-0.003258</td>\n",
              "      <td>-0.066790</td>\n",
              "      <td>-0.244832</td>\n",
              "      <td>0.030390</td>\n",
              "      <td>0.191575</td>\n",
              "      <td>0.024952</td>\n",
              "      <td>0.215342</td>\n",
              "      <td>-0.074820</td>\n",
              "      <td>0.043747</td>\n",
              "      <td>0.193661</td>\n",
              "      <td>-0.079593</td>\n",
              "      <td>-0.149580</td>\n",
              "      <td>0.061730</td>\n",
              "      <td>-0.005052</td>\n",
              "      <td>-0.147827</td>\n",
              "      <td>-0.700040</td>\n",
              "      <td>0.165868</td>\n",
              "      <td>-0.046675</td>\n",
              "      <td>-0.265952</td>\n",
              "      <td>-0.267837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007578</td>\n",
              "      <td>-0.091531</td>\n",
              "      <td>-0.291598</td>\n",
              "      <td>-0.068264</td>\n",
              "      <td>0.008742</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>-0.058632</td>\n",
              "      <td>-0.246112</td>\n",
              "      <td>0.218130</td>\n",
              "      <td>-0.016450</td>\n",
              "      <td>0.004930</td>\n",
              "      <td>-0.048359</td>\n",
              "      <td>0.124874</td>\n",
              "      <td>0.333840</td>\n",
              "      <td>-0.197837</td>\n",
              "      <td>0.525809</td>\n",
              "      <td>-1.544100</td>\n",
              "      <td>-0.024925</td>\n",
              "      <td>0.062168</td>\n",
              "      <td>0.020237</td>\n",
              "      <td>-0.218701</td>\n",
              "      <td>0.089318</td>\n",
              "      <td>0.091753</td>\n",
              "      <td>-0.120384</td>\n",
              "      <td>-0.108414</td>\n",
              "      <td>-0.028071</td>\n",
              "      <td>-0.230014</td>\n",
              "      <td>-0.023561</td>\n",
              "      <td>-0.042278</td>\n",
              "      <td>-0.110954</td>\n",
              "      <td>-0.168018</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>-0.075568</td>\n",
              "      <td>-0.118220</td>\n",
              "      <td>0.475009</td>\n",
              "      <td>0.345620</td>\n",
              "      <td>-0.062680</td>\n",
              "      <td>-0.013058</td>\n",
              "      <td>-0.346789</td>\n",
              "      <td>-0.184302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.293154</td>\n",
              "      <td>-0.076427</td>\n",
              "      <td>-0.154668</td>\n",
              "      <td>-0.059217</td>\n",
              "      <td>-0.059002</td>\n",
              "      <td>0.278274</td>\n",
              "      <td>-0.014315</td>\n",
              "      <td>0.197191</td>\n",
              "      <td>0.049958</td>\n",
              "      <td>-0.786268</td>\n",
              "      <td>0.266176</td>\n",
              "      <td>0.110769</td>\n",
              "      <td>-0.089363</td>\n",
              "      <td>0.001999</td>\n",
              "      <td>0.111346</td>\n",
              "      <td>0.092343</td>\n",
              "      <td>0.248268</td>\n",
              "      <td>-0.146874</td>\n",
              "      <td>0.039370</td>\n",
              "      <td>-0.013702</td>\n",
              "      <td>0.291454</td>\n",
              "      <td>0.045309</td>\n",
              "      <td>0.145594</td>\n",
              "      <td>0.156029</td>\n",
              "      <td>-0.190869</td>\n",
              "      <td>0.172915</td>\n",
              "      <td>-0.059751</td>\n",
              "      <td>-0.151784</td>\n",
              "      <td>-0.003540</td>\n",
              "      <td>-0.142450</td>\n",
              "      <td>0.088757</td>\n",
              "      <td>0.444503</td>\n",
              "      <td>-0.180326</td>\n",
              "      <td>-0.069164</td>\n",
              "      <td>-0.171736</td>\n",
              "      <td>0.190470</td>\n",
              "      <td>-0.034064</td>\n",
              "      <td>0.038189</td>\n",
              "      <td>0.106640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.117089</td>\n",
              "      <td>-0.152083</td>\n",
              "      <td>-0.164410</td>\n",
              "      <td>-0.106623</td>\n",
              "      <td>-0.178220</td>\n",
              "      <td>0.225706</td>\n",
              "      <td>-0.042528</td>\n",
              "      <td>-0.363157</td>\n",
              "      <td>0.286871</td>\n",
              "      <td>-0.009287</td>\n",
              "      <td>0.068377</td>\n",
              "      <td>0.148473</td>\n",
              "      <td>-0.162717</td>\n",
              "      <td>0.293580</td>\n",
              "      <td>-0.335430</td>\n",
              "      <td>0.194250</td>\n",
              "      <td>-0.129697</td>\n",
              "      <td>0.015882</td>\n",
              "      <td>-0.400536</td>\n",
              "      <td>0.132880</td>\n",
              "      <td>-0.135337</td>\n",
              "      <td>-0.175592</td>\n",
              "      <td>-0.097263</td>\n",
              "      <td>-0.037669</td>\n",
              "      <td>-0.121970</td>\n",
              "      <td>-0.435520</td>\n",
              "      <td>-0.149193</td>\n",
              "      <td>-0.058346</td>\n",
              "      <td>0.081077</td>\n",
              "      <td>-0.402637</td>\n",
              "      <td>-0.092627</td>\n",
              "      <td>-0.062974</td>\n",
              "      <td>0.263365</td>\n",
              "      <td>-0.012204</td>\n",
              "      <td>0.098552</td>\n",
              "      <td>-0.193720</td>\n",
              "      <td>0.380953</td>\n",
              "      <td>-0.360287</td>\n",
              "      <td>0.137164</td>\n",
              "      <td>0.173387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.120708</td>\n",
              "      <td>0.012485</td>\n",
              "      <td>-0.084866</td>\n",
              "      <td>-0.157200</td>\n",
              "      <td>-0.081759</td>\n",
              "      <td>-0.095468</td>\n",
              "      <td>-0.085420</td>\n",
              "      <td>0.155715</td>\n",
              "      <td>0.074075</td>\n",
              "      <td>-0.941932</td>\n",
              "      <td>0.112102</td>\n",
              "      <td>0.024208</td>\n",
              "      <td>0.017980</td>\n",
              "      <td>-0.105131</td>\n",
              "      <td>-0.318010</td>\n",
              "      <td>-0.039927</td>\n",
              "      <td>0.039036</td>\n",
              "      <td>-0.101775</td>\n",
              "      <td>-0.069636</td>\n",
              "      <td>-0.233389</td>\n",
              "      <td>-0.228755</td>\n",
              "      <td>0.103719</td>\n",
              "      <td>0.152153</td>\n",
              "      <td>0.020319</td>\n",
              "      <td>-0.179047</td>\n",
              "      <td>-0.229925</td>\n",
              "      <td>0.011305</td>\n",
              "      <td>0.058682</td>\n",
              "      <td>-0.010918</td>\n",
              "      <td>0.309250</td>\n",
              "      <td>0.156871</td>\n",
              "      <td>-0.047273</td>\n",
              "      <td>0.064185</td>\n",
              "      <td>0.096737</td>\n",
              "      <td>-0.324866</td>\n",
              "      <td>0.256432</td>\n",
              "      <td>-0.119297</td>\n",
              "      <td>-0.374670</td>\n",
              "      <td>-0.026976</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046861</td>\n",
              "      <td>0.050190</td>\n",
              "      <td>0.062310</td>\n",
              "      <td>0.242104</td>\n",
              "      <td>0.187998</td>\n",
              "      <td>-0.163162</td>\n",
              "      <td>-0.149218</td>\n",
              "      <td>0.090508</td>\n",
              "      <td>0.053851</td>\n",
              "      <td>-0.090001</td>\n",
              "      <td>-0.120537</td>\n",
              "      <td>0.151753</td>\n",
              "      <td>-0.016855</td>\n",
              "      <td>0.134122</td>\n",
              "      <td>-0.248188</td>\n",
              "      <td>0.112005</td>\n",
              "      <td>-0.586151</td>\n",
              "      <td>0.139937</td>\n",
              "      <td>0.233480</td>\n",
              "      <td>0.018990</td>\n",
              "      <td>-0.239051</td>\n",
              "      <td>0.084330</td>\n",
              "      <td>-0.370279</td>\n",
              "      <td>0.097975</td>\n",
              "      <td>0.125676</td>\n",
              "      <td>-0.073724</td>\n",
              "      <td>-0.075676</td>\n",
              "      <td>0.143606</td>\n",
              "      <td>0.113822</td>\n",
              "      <td>0.251213</td>\n",
              "      <td>0.364641</td>\n",
              "      <td>-0.155353</td>\n",
              "      <td>-0.191636</td>\n",
              "      <td>0.023351</td>\n",
              "      <td>0.006381</td>\n",
              "      <td>-0.106198</td>\n",
              "      <td>0.038931</td>\n",
              "      <td>-0.005640</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>-0.132440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.402403</td>\n",
              "      <td>-0.168747</td>\n",
              "      <td>0.150591</td>\n",
              "      <td>-0.276718</td>\n",
              "      <td>0.146649</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>-0.432730</td>\n",
              "      <td>-0.137463</td>\n",
              "      <td>-1.579967</td>\n",
              "      <td>0.107133</td>\n",
              "      <td>0.106472</td>\n",
              "      <td>-0.225510</td>\n",
              "      <td>0.363420</td>\n",
              "      <td>0.288383</td>\n",
              "      <td>-0.005712</td>\n",
              "      <td>-0.178427</td>\n",
              "      <td>0.025010</td>\n",
              "      <td>0.140244</td>\n",
              "      <td>0.049824</td>\n",
              "      <td>-0.159559</td>\n",
              "      <td>0.435483</td>\n",
              "      <td>0.082506</td>\n",
              "      <td>0.127247</td>\n",
              "      <td>-0.498813</td>\n",
              "      <td>-0.035577</td>\n",
              "      <td>0.163044</td>\n",
              "      <td>-0.321000</td>\n",
              "      <td>0.053461</td>\n",
              "      <td>-0.260323</td>\n",
              "      <td>0.011077</td>\n",
              "      <td>0.117706</td>\n",
              "      <td>-0.276837</td>\n",
              "      <td>-0.305683</td>\n",
              "      <td>-0.767600</td>\n",
              "      <td>0.174055</td>\n",
              "      <td>-0.040510</td>\n",
              "      <td>-0.146608</td>\n",
              "      <td>-0.086315</td>\n",
              "      <td>...</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>-0.220975</td>\n",
              "      <td>-0.248665</td>\n",
              "      <td>-0.015262</td>\n",
              "      <td>0.086361</td>\n",
              "      <td>0.291582</td>\n",
              "      <td>0.156888</td>\n",
              "      <td>0.208077</td>\n",
              "      <td>0.511552</td>\n",
              "      <td>0.300493</td>\n",
              "      <td>0.168303</td>\n",
              "      <td>-0.403020</td>\n",
              "      <td>-0.118676</td>\n",
              "      <td>-0.073964</td>\n",
              "      <td>0.154756</td>\n",
              "      <td>-0.211824</td>\n",
              "      <td>-0.092012</td>\n",
              "      <td>-0.117538</td>\n",
              "      <td>-0.103871</td>\n",
              "      <td>-0.081810</td>\n",
              "      <td>-0.081461</td>\n",
              "      <td>-0.124442</td>\n",
              "      <td>0.042128</td>\n",
              "      <td>-0.098186</td>\n",
              "      <td>-0.005035</td>\n",
              "      <td>-0.214217</td>\n",
              "      <td>0.212308</td>\n",
              "      <td>-0.169194</td>\n",
              "      <td>0.036502</td>\n",
              "      <td>-0.083160</td>\n",
              "      <td>0.106163</td>\n",
              "      <td>0.324990</td>\n",
              "      <td>-0.018780</td>\n",
              "      <td>-0.346085</td>\n",
              "      <td>0.417951</td>\n",
              "      <td>-0.005268</td>\n",
              "      <td>-0.022219</td>\n",
              "      <td>-0.315493</td>\n",
              "      <td>0.067823</td>\n",
              "      <td>-0.010444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.325340</td>\n",
              "      <td>0.217111</td>\n",
              "      <td>0.267070</td>\n",
              "      <td>-0.306210</td>\n",
              "      <td>-0.158905</td>\n",
              "      <td>-0.164993</td>\n",
              "      <td>0.157609</td>\n",
              "      <td>0.371457</td>\n",
              "      <td>0.197530</td>\n",
              "      <td>-1.630070</td>\n",
              "      <td>0.147243</td>\n",
              "      <td>-0.121327</td>\n",
              "      <td>0.252770</td>\n",
              "      <td>-0.139895</td>\n",
              "      <td>0.139031</td>\n",
              "      <td>0.068265</td>\n",
              "      <td>0.250888</td>\n",
              "      <td>-0.057495</td>\n",
              "      <td>0.096359</td>\n",
              "      <td>-0.246751</td>\n",
              "      <td>0.295695</td>\n",
              "      <td>-0.498655</td>\n",
              "      <td>0.178315</td>\n",
              "      <td>0.360510</td>\n",
              "      <td>-0.041090</td>\n",
              "      <td>-0.124743</td>\n",
              "      <td>-0.193690</td>\n",
              "      <td>0.303410</td>\n",
              "      <td>0.018360</td>\n",
              "      <td>0.281225</td>\n",
              "      <td>-0.103441</td>\n",
              "      <td>0.365440</td>\n",
              "      <td>0.009085</td>\n",
              "      <td>0.050800</td>\n",
              "      <td>-0.179165</td>\n",
              "      <td>0.169513</td>\n",
              "      <td>-0.218585</td>\n",
              "      <td>-0.377465</td>\n",
              "      <td>0.138154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302870</td>\n",
              "      <td>0.037740</td>\n",
              "      <td>-0.022663</td>\n",
              "      <td>0.196013</td>\n",
              "      <td>0.008974</td>\n",
              "      <td>0.155366</td>\n",
              "      <td>-0.190506</td>\n",
              "      <td>-0.070870</td>\n",
              "      <td>0.611433</td>\n",
              "      <td>0.039644</td>\n",
              "      <td>-0.095728</td>\n",
              "      <td>0.261176</td>\n",
              "      <td>0.015520</td>\n",
              "      <td>-0.189213</td>\n",
              "      <td>-0.014632</td>\n",
              "      <td>0.435720</td>\n",
              "      <td>-1.009016</td>\n",
              "      <td>-0.224986</td>\n",
              "      <td>-0.383780</td>\n",
              "      <td>-0.350933</td>\n",
              "      <td>0.041650</td>\n",
              "      <td>-0.030313</td>\n",
              "      <td>-0.394696</td>\n",
              "      <td>-0.156812</td>\n",
              "      <td>0.298708</td>\n",
              "      <td>-0.020957</td>\n",
              "      <td>-0.220085</td>\n",
              "      <td>0.130020</td>\n",
              "      <td>-0.047850</td>\n",
              "      <td>-0.001577</td>\n",
              "      <td>0.027228</td>\n",
              "      <td>-0.406904</td>\n",
              "      <td>-0.287423</td>\n",
              "      <td>-0.166498</td>\n",
              "      <td>-0.030453</td>\n",
              "      <td>0.355320</td>\n",
              "      <td>0.017057</td>\n",
              "      <td>0.015928</td>\n",
              "      <td>0.227331</td>\n",
              "      <td>-0.188086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.244914</td>\n",
              "      <td>0.093662</td>\n",
              "      <td>-0.027757</td>\n",
              "      <td>-0.066640</td>\n",
              "      <td>-0.158335</td>\n",
              "      <td>-0.023064</td>\n",
              "      <td>-0.175089</td>\n",
              "      <td>0.048242</td>\n",
              "      <td>0.216452</td>\n",
              "      <td>-1.508600</td>\n",
              "      <td>0.112322</td>\n",
              "      <td>0.030818</td>\n",
              "      <td>0.083644</td>\n",
              "      <td>-0.082905</td>\n",
              "      <td>0.039925</td>\n",
              "      <td>0.058427</td>\n",
              "      <td>-0.181431</td>\n",
              "      <td>0.014153</td>\n",
              "      <td>-0.093081</td>\n",
              "      <td>-0.123655</td>\n",
              "      <td>0.058087</td>\n",
              "      <td>-0.056250</td>\n",
              "      <td>0.117923</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>-0.088613</td>\n",
              "      <td>-0.033435</td>\n",
              "      <td>-0.050215</td>\n",
              "      <td>-0.061598</td>\n",
              "      <td>-0.036681</td>\n",
              "      <td>0.052545</td>\n",
              "      <td>-0.019356</td>\n",
              "      <td>0.176191</td>\n",
              "      <td>-0.078447</td>\n",
              "      <td>0.116948</td>\n",
              "      <td>-0.542039</td>\n",
              "      <td>0.104669</td>\n",
              "      <td>-0.020710</td>\n",
              "      <td>-0.146428</td>\n",
              "      <td>-0.062691</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.086308</td>\n",
              "      <td>0.127760</td>\n",
              "      <td>-0.081284</td>\n",
              "      <td>-0.068839</td>\n",
              "      <td>-0.073137</td>\n",
              "      <td>-0.024871</td>\n",
              "      <td>-0.066076</td>\n",
              "      <td>0.168531</td>\n",
              "      <td>0.021775</td>\n",
              "      <td>-0.099072</td>\n",
              "      <td>-0.285511</td>\n",
              "      <td>0.029763</td>\n",
              "      <td>-0.014819</td>\n",
              "      <td>0.078576</td>\n",
              "      <td>-0.123324</td>\n",
              "      <td>0.086437</td>\n",
              "      <td>-1.260673</td>\n",
              "      <td>0.255894</td>\n",
              "      <td>0.065157</td>\n",
              "      <td>0.048973</td>\n",
              "      <td>-0.083221</td>\n",
              "      <td>0.027670</td>\n",
              "      <td>-0.579514</td>\n",
              "      <td>0.031090</td>\n",
              "      <td>-0.011083</td>\n",
              "      <td>0.072131</td>\n",
              "      <td>-0.169337</td>\n",
              "      <td>0.145191</td>\n",
              "      <td>0.135919</td>\n",
              "      <td>-0.041317</td>\n",
              "      <td>0.182007</td>\n",
              "      <td>-0.351223</td>\n",
              "      <td>-0.091991</td>\n",
              "      <td>0.068461</td>\n",
              "      <td>0.155371</td>\n",
              "      <td>0.103223</td>\n",
              "      <td>-0.175219</td>\n",
              "      <td>-0.377766</td>\n",
              "      <td>-0.108440</td>\n",
              "      <td>-0.057285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0.027837</td>\n",
              "      <td>0.020295</td>\n",
              "      <td>-0.151023</td>\n",
              "      <td>-0.095898</td>\n",
              "      <td>-0.167358</td>\n",
              "      <td>0.084093</td>\n",
              "      <td>0.031332</td>\n",
              "      <td>0.029130</td>\n",
              "      <td>-0.153063</td>\n",
              "      <td>-0.867481</td>\n",
              "      <td>0.149270</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.188919</td>\n",
              "      <td>0.123589</td>\n",
              "      <td>-0.138419</td>\n",
              "      <td>0.088863</td>\n",
              "      <td>-0.071667</td>\n",
              "      <td>0.145525</td>\n",
              "      <td>-0.174678</td>\n",
              "      <td>-0.307153</td>\n",
              "      <td>0.039379</td>\n",
              "      <td>-0.332565</td>\n",
              "      <td>-0.112367</td>\n",
              "      <td>0.013726</td>\n",
              "      <td>0.049264</td>\n",
              "      <td>-0.169036</td>\n",
              "      <td>0.187930</td>\n",
              "      <td>0.045053</td>\n",
              "      <td>0.180780</td>\n",
              "      <td>0.128321</td>\n",
              "      <td>0.195320</td>\n",
              "      <td>0.126824</td>\n",
              "      <td>0.133917</td>\n",
              "      <td>0.293786</td>\n",
              "      <td>-0.775549</td>\n",
              "      <td>-0.128404</td>\n",
              "      <td>-0.000768</td>\n",
              "      <td>0.099696</td>\n",
              "      <td>-0.039367</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055322</td>\n",
              "      <td>0.058512</td>\n",
              "      <td>-0.013962</td>\n",
              "      <td>-0.030244</td>\n",
              "      <td>0.112004</td>\n",
              "      <td>-0.211126</td>\n",
              "      <td>0.103709</td>\n",
              "      <td>-0.035720</td>\n",
              "      <td>0.307869</td>\n",
              "      <td>-0.096432</td>\n",
              "      <td>0.121801</td>\n",
              "      <td>0.022704</td>\n",
              "      <td>0.090784</td>\n",
              "      <td>0.174238</td>\n",
              "      <td>0.119201</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>-1.474712</td>\n",
              "      <td>-0.027708</td>\n",
              "      <td>0.200302</td>\n",
              "      <td>0.161338</td>\n",
              "      <td>-0.221562</td>\n",
              "      <td>-0.099191</td>\n",
              "      <td>0.165827</td>\n",
              "      <td>-0.121238</td>\n",
              "      <td>0.054081</td>\n",
              "      <td>-0.021115</td>\n",
              "      <td>-0.153796</td>\n",
              "      <td>-0.141276</td>\n",
              "      <td>-0.113452</td>\n",
              "      <td>0.075824</td>\n",
              "      <td>-0.145704</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>-0.192792</td>\n",
              "      <td>0.062866</td>\n",
              "      <td>-0.209677</td>\n",
              "      <td>0.222916</td>\n",
              "      <td>-0.096914</td>\n",
              "      <td>-0.200379</td>\n",
              "      <td>-0.069198</td>\n",
              "      <td>-0.068444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.412964</td>\n",
              "      <td>-0.012519</td>\n",
              "      <td>0.028251</td>\n",
              "      <td>-0.188487</td>\n",
              "      <td>0.089138</td>\n",
              "      <td>0.203219</td>\n",
              "      <td>-0.010108</td>\n",
              "      <td>0.099409</td>\n",
              "      <td>0.064276</td>\n",
              "      <td>-0.981763</td>\n",
              "      <td>0.214659</td>\n",
              "      <td>0.011479</td>\n",
              "      <td>0.377556</td>\n",
              "      <td>-0.164959</td>\n",
              "      <td>-0.088536</td>\n",
              "      <td>0.063706</td>\n",
              "      <td>-0.065152</td>\n",
              "      <td>-0.229919</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>-0.110009</td>\n",
              "      <td>-0.055289</td>\n",
              "      <td>0.033349</td>\n",
              "      <td>0.219180</td>\n",
              "      <td>0.143817</td>\n",
              "      <td>-0.100256</td>\n",
              "      <td>-0.080379</td>\n",
              "      <td>-0.134950</td>\n",
              "      <td>0.252184</td>\n",
              "      <td>0.185287</td>\n",
              "      <td>0.113546</td>\n",
              "      <td>0.030238</td>\n",
              "      <td>0.233300</td>\n",
              "      <td>-0.002710</td>\n",
              "      <td>0.036019</td>\n",
              "      <td>-0.419412</td>\n",
              "      <td>0.396053</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.177725</td>\n",
              "      <td>0.028153</td>\n",
              "      <td>...</td>\n",
              "      <td>0.105347</td>\n",
              "      <td>0.140270</td>\n",
              "      <td>-0.402370</td>\n",
              "      <td>0.485162</td>\n",
              "      <td>0.427307</td>\n",
              "      <td>0.037149</td>\n",
              "      <td>-0.155085</td>\n",
              "      <td>-0.212613</td>\n",
              "      <td>-0.021569</td>\n",
              "      <td>0.222713</td>\n",
              "      <td>0.062682</td>\n",
              "      <td>0.057920</td>\n",
              "      <td>-0.196107</td>\n",
              "      <td>0.042997</td>\n",
              "      <td>-0.414487</td>\n",
              "      <td>0.056173</td>\n",
              "      <td>-0.059867</td>\n",
              "      <td>-0.153022</td>\n",
              "      <td>-0.295600</td>\n",
              "      <td>0.134444</td>\n",
              "      <td>-0.332067</td>\n",
              "      <td>0.030867</td>\n",
              "      <td>-0.347590</td>\n",
              "      <td>0.160020</td>\n",
              "      <td>-0.009089</td>\n",
              "      <td>0.197637</td>\n",
              "      <td>0.027307</td>\n",
              "      <td>0.204514</td>\n",
              "      <td>-0.027754</td>\n",
              "      <td>0.124833</td>\n",
              "      <td>0.081247</td>\n",
              "      <td>-0.139187</td>\n",
              "      <td>0.630633</td>\n",
              "      <td>0.251217</td>\n",
              "      <td>-0.086640</td>\n",
              "      <td>0.044171</td>\n",
              "      <td>-0.109899</td>\n",
              "      <td>-0.295923</td>\n",
              "      <td>0.135444</td>\n",
              "      <td>-0.471450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.279828</td>\n",
              "      <td>-0.052270</td>\n",
              "      <td>-0.056591</td>\n",
              "      <td>-0.243003</td>\n",
              "      <td>-0.045230</td>\n",
              "      <td>0.068209</td>\n",
              "      <td>-0.079466</td>\n",
              "      <td>-0.024867</td>\n",
              "      <td>0.070908</td>\n",
              "      <td>-1.677698</td>\n",
              "      <td>0.178797</td>\n",
              "      <td>-0.022957</td>\n",
              "      <td>-0.043643</td>\n",
              "      <td>0.034298</td>\n",
              "      <td>-0.018536</td>\n",
              "      <td>-0.149635</td>\n",
              "      <td>-0.007092</td>\n",
              "      <td>-0.122470</td>\n",
              "      <td>-0.028320</td>\n",
              "      <td>-0.137960</td>\n",
              "      <td>0.030926</td>\n",
              "      <td>0.255761</td>\n",
              "      <td>0.060751</td>\n",
              "      <td>0.371761</td>\n",
              "      <td>-0.051812</td>\n",
              "      <td>0.079794</td>\n",
              "      <td>0.176869</td>\n",
              "      <td>0.039610</td>\n",
              "      <td>-0.181804</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>0.072170</td>\n",
              "      <td>0.076428</td>\n",
              "      <td>-0.333967</td>\n",
              "      <td>0.097840</td>\n",
              "      <td>-0.766152</td>\n",
              "      <td>0.179611</td>\n",
              "      <td>-0.028031</td>\n",
              "      <td>-0.172853</td>\n",
              "      <td>-0.078061</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035368</td>\n",
              "      <td>-0.390056</td>\n",
              "      <td>-0.109151</td>\n",
              "      <td>-0.083273</td>\n",
              "      <td>0.152746</td>\n",
              "      <td>-0.096855</td>\n",
              "      <td>0.148468</td>\n",
              "      <td>0.005404</td>\n",
              "      <td>0.126428</td>\n",
              "      <td>-0.156779</td>\n",
              "      <td>0.023159</td>\n",
              "      <td>0.153470</td>\n",
              "      <td>-0.228604</td>\n",
              "      <td>-0.194155</td>\n",
              "      <td>-0.134939</td>\n",
              "      <td>0.210125</td>\n",
              "      <td>-0.975126</td>\n",
              "      <td>0.014408</td>\n",
              "      <td>-0.017470</td>\n",
              "      <td>0.035917</td>\n",
              "      <td>-0.086836</td>\n",
              "      <td>-0.033957</td>\n",
              "      <td>0.058749</td>\n",
              "      <td>0.176562</td>\n",
              "      <td>-0.007530</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.128723</td>\n",
              "      <td>0.059782</td>\n",
              "      <td>0.074716</td>\n",
              "      <td>-0.095617</td>\n",
              "      <td>0.141917</td>\n",
              "      <td>-0.118375</td>\n",
              "      <td>-0.029259</td>\n",
              "      <td>-0.074191</td>\n",
              "      <td>-0.138909</td>\n",
              "      <td>-0.043277</td>\n",
              "      <td>-0.046244</td>\n",
              "      <td>-0.032247</td>\n",
              "      <td>-0.045748</td>\n",
              "      <td>-0.074187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.322386</td>\n",
              "      <td>0.047771</td>\n",
              "      <td>-0.167572</td>\n",
              "      <td>-0.151986</td>\n",
              "      <td>0.038950</td>\n",
              "      <td>0.020654</td>\n",
              "      <td>-0.016936</td>\n",
              "      <td>0.056299</td>\n",
              "      <td>-0.120633</td>\n",
              "      <td>-1.072441</td>\n",
              "      <td>0.054033</td>\n",
              "      <td>0.104509</td>\n",
              "      <td>-0.283115</td>\n",
              "      <td>0.237898</td>\n",
              "      <td>0.024727</td>\n",
              "      <td>0.041248</td>\n",
              "      <td>-0.240467</td>\n",
              "      <td>0.068651</td>\n",
              "      <td>0.251415</td>\n",
              "      <td>-0.211827</td>\n",
              "      <td>-0.293594</td>\n",
              "      <td>-0.247800</td>\n",
              "      <td>0.079407</td>\n",
              "      <td>0.151314</td>\n",
              "      <td>-0.036017</td>\n",
              "      <td>0.017469</td>\n",
              "      <td>0.081848</td>\n",
              "      <td>-0.170676</td>\n",
              "      <td>-0.140587</td>\n",
              "      <td>0.031396</td>\n",
              "      <td>-0.141419</td>\n",
              "      <td>0.160582</td>\n",
              "      <td>0.006981</td>\n",
              "      <td>0.079395</td>\n",
              "      <td>-0.583176</td>\n",
              "      <td>-0.006178</td>\n",
              "      <td>0.262317</td>\n",
              "      <td>0.150346</td>\n",
              "      <td>-0.147513</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154757</td>\n",
              "      <td>-0.144909</td>\n",
              "      <td>0.134319</td>\n",
              "      <td>0.130963</td>\n",
              "      <td>0.043977</td>\n",
              "      <td>0.092827</td>\n",
              "      <td>0.188927</td>\n",
              "      <td>0.278433</td>\n",
              "      <td>-0.194429</td>\n",
              "      <td>0.289890</td>\n",
              "      <td>-0.206093</td>\n",
              "      <td>-0.238072</td>\n",
              "      <td>0.230767</td>\n",
              "      <td>0.243780</td>\n",
              "      <td>-0.292813</td>\n",
              "      <td>0.211943</td>\n",
              "      <td>0.244897</td>\n",
              "      <td>-0.027660</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>-0.012973</td>\n",
              "      <td>-0.149280</td>\n",
              "      <td>0.208323</td>\n",
              "      <td>-0.373501</td>\n",
              "      <td>-0.263112</td>\n",
              "      <td>0.449067</td>\n",
              "      <td>0.216937</td>\n",
              "      <td>0.477510</td>\n",
              "      <td>-0.326597</td>\n",
              "      <td>0.012518</td>\n",
              "      <td>0.262237</td>\n",
              "      <td>0.126132</td>\n",
              "      <td>-0.051155</td>\n",
              "      <td>-0.024783</td>\n",
              "      <td>-0.147943</td>\n",
              "      <td>-0.072380</td>\n",
              "      <td>-0.555348</td>\n",
              "      <td>-0.372659</td>\n",
              "      <td>0.139087</td>\n",
              "      <td>0.219664</td>\n",
              "      <td>0.084273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 3901 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Y    Body_1    Body_2    Body_3  ...  Title_297  Title_298  Title_299  Title_300\n",
              "0  0 -0.240987  0.294838  0.043283  ...  -0.062680  -0.013058  -0.346789  -0.184302\n",
              "1  2 -0.293154 -0.076427 -0.154668  ...   0.380953  -0.360287   0.137164   0.173387\n",
              "2  2 -0.120708  0.012485 -0.084866  ...   0.038931  -0.005640   0.000261  -0.132440\n",
              "3  2 -0.402403 -0.168747  0.150591  ...  -0.022219  -0.315493   0.067823  -0.010444\n",
              "4  2 -0.325340  0.217111  0.267070  ...   0.017057   0.015928   0.227331  -0.188086\n",
              "5  0 -0.244914  0.093662 -0.027757  ...  -0.175219  -0.377766  -0.108440  -0.057285\n",
              "6  1  0.027837  0.020295 -0.151023  ...  -0.096914  -0.200379  -0.069198  -0.068444\n",
              "7  1 -0.412964 -0.012519  0.028251  ...  -0.109899  -0.295923   0.135444  -0.471450\n",
              "8  2 -0.279828 -0.052270 -0.056591  ...  -0.046244  -0.032247  -0.045748  -0.074187\n",
              "9  1 -0.322386  0.047771 -0.167572  ...  -0.372659   0.139087   0.219664   0.084273\n",
              "\n",
              "[10 rows x 3901 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80AVyuu_18bs"
      },
      "source": [
        "Normalising the data using standard scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDae5PmcTVPX"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler = preprocessing.StandardScaler() #for data normalization\n",
        "cols= list(d.columns.values)\n",
        "\n",
        "scaler.fit(d[cols])\n",
        "X = pd.DataFrame(scaler.transform(d[cols]))\n",
        "y = pd.DataFrame(d['Y'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1czSau2IBO"
      },
      "source": [
        "Importing the test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK1u0CSLSjpr"
      },
      "source": [
        "td = pd.read_csv('/content/drive/MyDrive/EE769Project/glove_sent_encoding_Testdata_12Sent.csv')\n",
        "td.drop([\"Unnamed: 0\",\"Id\",\"CreationDate\",\"Code_blocks\",\"Num_Tags\",\"Body_size\",\"Extra_tags\",\"Code_Lines\",\"Num Tags\"],axis=1,inplace=True)\n",
        "# To convert string to numeric value\n",
        "new_values = {'HQ':2, 'LQ_EDIT':1, 'LQ_CLOSE':0}\n",
        "td['Y']  = td.Y.replace(new_values)\n",
        "\n",
        "#Normalize test data using the same scaler\n",
        "\n",
        "cols= list(td.columns.values)\n",
        "validX = pd.DataFrame(scaler.transform(td[cols]))\n",
        "validY = pd.DataFrame(td['Y'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "IwlVCZ1ZIV2l",
        "outputId": "4dec5692-6425-4408-bc90-ee07ea9835b2"
      },
      "source": [
        "display(td)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>Body_1</th>\n",
              "      <th>Body_2</th>\n",
              "      <th>Body_3</th>\n",
              "      <th>Body_4</th>\n",
              "      <th>Body_5</th>\n",
              "      <th>Body_6</th>\n",
              "      <th>Body_7</th>\n",
              "      <th>Body_8</th>\n",
              "      <th>Body_9</th>\n",
              "      <th>Body_10</th>\n",
              "      <th>Body_11</th>\n",
              "      <th>Body_12</th>\n",
              "      <th>Body_13</th>\n",
              "      <th>Body_14</th>\n",
              "      <th>Body_15</th>\n",
              "      <th>Body_16</th>\n",
              "      <th>Body_17</th>\n",
              "      <th>Body_18</th>\n",
              "      <th>Body_19</th>\n",
              "      <th>Body_20</th>\n",
              "      <th>Body_21</th>\n",
              "      <th>Body_22</th>\n",
              "      <th>Body_23</th>\n",
              "      <th>Body_24</th>\n",
              "      <th>Body_25</th>\n",
              "      <th>Body_26</th>\n",
              "      <th>Body_27</th>\n",
              "      <th>Body_28</th>\n",
              "      <th>Body_29</th>\n",
              "      <th>Body_30</th>\n",
              "      <th>Body_31</th>\n",
              "      <th>Body_32</th>\n",
              "      <th>Body_33</th>\n",
              "      <th>Body_34</th>\n",
              "      <th>Body_35</th>\n",
              "      <th>Body_36</th>\n",
              "      <th>Body_37</th>\n",
              "      <th>Body_38</th>\n",
              "      <th>Body_39</th>\n",
              "      <th>...</th>\n",
              "      <th>Title_261</th>\n",
              "      <th>Title_262</th>\n",
              "      <th>Title_263</th>\n",
              "      <th>Title_264</th>\n",
              "      <th>Title_265</th>\n",
              "      <th>Title_266</th>\n",
              "      <th>Title_267</th>\n",
              "      <th>Title_268</th>\n",
              "      <th>Title_269</th>\n",
              "      <th>Title_270</th>\n",
              "      <th>Title_271</th>\n",
              "      <th>Title_272</th>\n",
              "      <th>Title_273</th>\n",
              "      <th>Title_274</th>\n",
              "      <th>Title_275</th>\n",
              "      <th>Title_276</th>\n",
              "      <th>Title_277</th>\n",
              "      <th>Title_278</th>\n",
              "      <th>Title_279</th>\n",
              "      <th>Title_280</th>\n",
              "      <th>Title_281</th>\n",
              "      <th>Title_282</th>\n",
              "      <th>Title_283</th>\n",
              "      <th>Title_284</th>\n",
              "      <th>Title_285</th>\n",
              "      <th>Title_286</th>\n",
              "      <th>Title_287</th>\n",
              "      <th>Title_288</th>\n",
              "      <th>Title_289</th>\n",
              "      <th>Title_290</th>\n",
              "      <th>Title_291</th>\n",
              "      <th>Title_292</th>\n",
              "      <th>Title_293</th>\n",
              "      <th>Title_294</th>\n",
              "      <th>Title_295</th>\n",
              "      <th>Title_296</th>\n",
              "      <th>Title_297</th>\n",
              "      <th>Title_298</th>\n",
              "      <th>Title_299</th>\n",
              "      <th>Title_300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.340877</td>\n",
              "      <td>0.215006</td>\n",
              "      <td>0.042059</td>\n",
              "      <td>0.044235</td>\n",
              "      <td>-0.095466</td>\n",
              "      <td>0.196629</td>\n",
              "      <td>-0.003774</td>\n",
              "      <td>0.159970</td>\n",
              "      <td>0.004629</td>\n",
              "      <td>-0.897817</td>\n",
              "      <td>0.053149</td>\n",
              "      <td>0.057117</td>\n",
              "      <td>-0.041261</td>\n",
              "      <td>0.015571</td>\n",
              "      <td>-0.038483</td>\n",
              "      <td>-0.217230</td>\n",
              "      <td>-0.127944</td>\n",
              "      <td>-0.165420</td>\n",
              "      <td>-0.000777</td>\n",
              "      <td>-0.174952</td>\n",
              "      <td>-0.037054</td>\n",
              "      <td>0.064127</td>\n",
              "      <td>-0.158586</td>\n",
              "      <td>0.124899</td>\n",
              "      <td>-0.043402</td>\n",
              "      <td>0.067632</td>\n",
              "      <td>-0.093716</td>\n",
              "      <td>0.159732</td>\n",
              "      <td>0.130028</td>\n",
              "      <td>-0.123124</td>\n",
              "      <td>0.049366</td>\n",
              "      <td>0.033423</td>\n",
              "      <td>-0.118762</td>\n",
              "      <td>-0.044195</td>\n",
              "      <td>-0.813885</td>\n",
              "      <td>0.117765</td>\n",
              "      <td>0.090156</td>\n",
              "      <td>0.028049</td>\n",
              "      <td>-0.406280</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.139323</td>\n",
              "      <td>-0.048838</td>\n",
              "      <td>-0.026537</td>\n",
              "      <td>0.077406</td>\n",
              "      <td>0.069879</td>\n",
              "      <td>0.187693</td>\n",
              "      <td>0.107893</td>\n",
              "      <td>0.027879</td>\n",
              "      <td>0.141835</td>\n",
              "      <td>0.081695</td>\n",
              "      <td>-0.226857</td>\n",
              "      <td>-0.043289</td>\n",
              "      <td>0.209849</td>\n",
              "      <td>-0.188163</td>\n",
              "      <td>-0.038189</td>\n",
              "      <td>-0.027231</td>\n",
              "      <td>-1.455375</td>\n",
              "      <td>0.072748</td>\n",
              "      <td>0.083586</td>\n",
              "      <td>0.053776</td>\n",
              "      <td>-0.188114</td>\n",
              "      <td>-0.073266</td>\n",
              "      <td>-0.190253</td>\n",
              "      <td>0.042722</td>\n",
              "      <td>-0.173995</td>\n",
              "      <td>0.109777</td>\n",
              "      <td>0.147412</td>\n",
              "      <td>0.099228</td>\n",
              "      <td>-0.023537</td>\n",
              "      <td>-0.395013</td>\n",
              "      <td>0.206241</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>-0.170344</td>\n",
              "      <td>-0.074469</td>\n",
              "      <td>0.060374</td>\n",
              "      <td>0.150742</td>\n",
              "      <td>0.023050</td>\n",
              "      <td>-0.090500</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.019357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.317487</td>\n",
              "      <td>0.283310</td>\n",
              "      <td>0.161350</td>\n",
              "      <td>0.007686</td>\n",
              "      <td>-0.391707</td>\n",
              "      <td>0.207225</td>\n",
              "      <td>-0.238494</td>\n",
              "      <td>0.076843</td>\n",
              "      <td>0.138330</td>\n",
              "      <td>-0.962733</td>\n",
              "      <td>0.369443</td>\n",
              "      <td>0.175665</td>\n",
              "      <td>-0.034254</td>\n",
              "      <td>0.416783</td>\n",
              "      <td>0.006655</td>\n",
              "      <td>0.298977</td>\n",
              "      <td>-0.362460</td>\n",
              "      <td>0.199014</td>\n",
              "      <td>-0.211890</td>\n",
              "      <td>-0.342117</td>\n",
              "      <td>-0.325649</td>\n",
              "      <td>-0.158180</td>\n",
              "      <td>-0.034711</td>\n",
              "      <td>-0.027403</td>\n",
              "      <td>0.048063</td>\n",
              "      <td>0.064993</td>\n",
              "      <td>-0.050820</td>\n",
              "      <td>0.057365</td>\n",
              "      <td>-0.116117</td>\n",
              "      <td>-0.323510</td>\n",
              "      <td>-0.391542</td>\n",
              "      <td>0.339480</td>\n",
              "      <td>-0.033733</td>\n",
              "      <td>0.352940</td>\n",
              "      <td>-1.249733</td>\n",
              "      <td>0.242970</td>\n",
              "      <td>0.389523</td>\n",
              "      <td>0.356480</td>\n",
              "      <td>-0.530697</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112984</td>\n",
              "      <td>-0.024103</td>\n",
              "      <td>0.116888</td>\n",
              "      <td>0.257347</td>\n",
              "      <td>0.183601</td>\n",
              "      <td>0.403038</td>\n",
              "      <td>-0.231670</td>\n",
              "      <td>0.032001</td>\n",
              "      <td>-0.001861</td>\n",
              "      <td>-0.088077</td>\n",
              "      <td>-0.438003</td>\n",
              "      <td>0.112986</td>\n",
              "      <td>0.229143</td>\n",
              "      <td>0.413923</td>\n",
              "      <td>0.075720</td>\n",
              "      <td>-0.407310</td>\n",
              "      <td>-1.610467</td>\n",
              "      <td>0.396207</td>\n",
              "      <td>0.296177</td>\n",
              "      <td>-0.293330</td>\n",
              "      <td>-0.456730</td>\n",
              "      <td>0.223468</td>\n",
              "      <td>-0.123125</td>\n",
              "      <td>-0.373383</td>\n",
              "      <td>0.161447</td>\n",
              "      <td>-0.399090</td>\n",
              "      <td>0.175964</td>\n",
              "      <td>0.041453</td>\n",
              "      <td>-0.055593</td>\n",
              "      <td>-0.000695</td>\n",
              "      <td>-0.011951</td>\n",
              "      <td>-0.147693</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.014210</td>\n",
              "      <td>0.190050</td>\n",
              "      <td>-0.160333</td>\n",
              "      <td>0.453571</td>\n",
              "      <td>-0.289137</td>\n",
              "      <td>-0.033799</td>\n",
              "      <td>-0.084037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.219626</td>\n",
              "      <td>0.050607</td>\n",
              "      <td>0.043197</td>\n",
              "      <td>-0.111386</td>\n",
              "      <td>-0.050845</td>\n",
              "      <td>0.038048</td>\n",
              "      <td>-0.194342</td>\n",
              "      <td>-0.056696</td>\n",
              "      <td>0.137598</td>\n",
              "      <td>-0.737625</td>\n",
              "      <td>0.224304</td>\n",
              "      <td>-0.008981</td>\n",
              "      <td>0.071190</td>\n",
              "      <td>-0.143245</td>\n",
              "      <td>-0.213485</td>\n",
              "      <td>0.163526</td>\n",
              "      <td>-0.115521</td>\n",
              "      <td>0.091554</td>\n",
              "      <td>-0.079014</td>\n",
              "      <td>0.074989</td>\n",
              "      <td>0.029614</td>\n",
              "      <td>-0.123970</td>\n",
              "      <td>0.058389</td>\n",
              "      <td>0.169670</td>\n",
              "      <td>-0.114695</td>\n",
              "      <td>0.017152</td>\n",
              "      <td>-0.100463</td>\n",
              "      <td>0.179791</td>\n",
              "      <td>-0.149299</td>\n",
              "      <td>0.198931</td>\n",
              "      <td>-0.149124</td>\n",
              "      <td>-0.033526</td>\n",
              "      <td>-0.053938</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.019931</td>\n",
              "      <td>-0.020367</td>\n",
              "      <td>0.084659</td>\n",
              "      <td>0.121586</td>\n",
              "      <td>-0.090234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.103640</td>\n",
              "      <td>-0.541410</td>\n",
              "      <td>-0.318410</td>\n",
              "      <td>-0.098462</td>\n",
              "      <td>0.130510</td>\n",
              "      <td>0.222980</td>\n",
              "      <td>-0.016344</td>\n",
              "      <td>0.168200</td>\n",
              "      <td>0.567350</td>\n",
              "      <td>-0.481430</td>\n",
              "      <td>-0.159240</td>\n",
              "      <td>-0.555060</td>\n",
              "      <td>-0.213160</td>\n",
              "      <td>-0.803490</td>\n",
              "      <td>0.044645</td>\n",
              "      <td>-0.709430</td>\n",
              "      <td>-0.294970</td>\n",
              "      <td>-0.511900</td>\n",
              "      <td>-0.366220</td>\n",
              "      <td>-0.351110</td>\n",
              "      <td>-0.161930</td>\n",
              "      <td>0.369920</td>\n",
              "      <td>-0.189060</td>\n",
              "      <td>0.368290</td>\n",
              "      <td>-0.972190</td>\n",
              "      <td>-0.129030</td>\n",
              "      <td>0.166290</td>\n",
              "      <td>0.653680</td>\n",
              "      <td>0.539990</td>\n",
              "      <td>-0.471150</td>\n",
              "      <td>0.178900</td>\n",
              "      <td>0.209660</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>-0.781370</td>\n",
              "      <td>-0.569280</td>\n",
              "      <td>-0.374350</td>\n",
              "      <td>-0.123720</td>\n",
              "      <td>0.051204</td>\n",
              "      <td>0.791200</td>\n",
              "      <td>-0.713580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.274443</td>\n",
              "      <td>0.032652</td>\n",
              "      <td>-0.069473</td>\n",
              "      <td>-0.181262</td>\n",
              "      <td>-0.059048</td>\n",
              "      <td>0.089089</td>\n",
              "      <td>0.064029</td>\n",
              "      <td>-0.012579</td>\n",
              "      <td>0.089457</td>\n",
              "      <td>-1.323730</td>\n",
              "      <td>0.046081</td>\n",
              "      <td>-0.069050</td>\n",
              "      <td>-0.187425</td>\n",
              "      <td>-0.073403</td>\n",
              "      <td>0.112012</td>\n",
              "      <td>-0.054203</td>\n",
              "      <td>-0.208339</td>\n",
              "      <td>-0.065831</td>\n",
              "      <td>0.205049</td>\n",
              "      <td>-0.108460</td>\n",
              "      <td>-0.046697</td>\n",
              "      <td>0.144407</td>\n",
              "      <td>0.160408</td>\n",
              "      <td>0.229873</td>\n",
              "      <td>-0.045338</td>\n",
              "      <td>0.062148</td>\n",
              "      <td>0.167209</td>\n",
              "      <td>0.295752</td>\n",
              "      <td>-0.001458</td>\n",
              "      <td>-0.054021</td>\n",
              "      <td>-0.122230</td>\n",
              "      <td>0.246369</td>\n",
              "      <td>-0.141188</td>\n",
              "      <td>0.031583</td>\n",
              "      <td>-0.630895</td>\n",
              "      <td>0.178795</td>\n",
              "      <td>-0.001612</td>\n",
              "      <td>-0.006151</td>\n",
              "      <td>0.069256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186280</td>\n",
              "      <td>0.035717</td>\n",
              "      <td>-0.150680</td>\n",
              "      <td>-0.203480</td>\n",
              "      <td>-0.338687</td>\n",
              "      <td>0.097367</td>\n",
              "      <td>0.166783</td>\n",
              "      <td>0.107827</td>\n",
              "      <td>0.192506</td>\n",
              "      <td>0.327670</td>\n",
              "      <td>-0.263653</td>\n",
              "      <td>-0.291083</td>\n",
              "      <td>-0.115321</td>\n",
              "      <td>-0.080212</td>\n",
              "      <td>-0.413057</td>\n",
              "      <td>-0.172063</td>\n",
              "      <td>-0.353060</td>\n",
              "      <td>-0.256941</td>\n",
              "      <td>-0.001917</td>\n",
              "      <td>0.197095</td>\n",
              "      <td>-0.098512</td>\n",
              "      <td>-0.183403</td>\n",
              "      <td>-0.629495</td>\n",
              "      <td>-0.212549</td>\n",
              "      <td>0.016530</td>\n",
              "      <td>-0.098406</td>\n",
              "      <td>-0.073957</td>\n",
              "      <td>-0.026804</td>\n",
              "      <td>-0.445486</td>\n",
              "      <td>-0.000705</td>\n",
              "      <td>-0.147647</td>\n",
              "      <td>-0.134659</td>\n",
              "      <td>0.041587</td>\n",
              "      <td>-0.086757</td>\n",
              "      <td>0.368247</td>\n",
              "      <td>-0.360973</td>\n",
              "      <td>0.034436</td>\n",
              "      <td>0.190240</td>\n",
              "      <td>0.212298</td>\n",
              "      <td>0.092343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.174143</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.184725</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>-0.079135</td>\n",
              "      <td>0.152766</td>\n",
              "      <td>0.135071</td>\n",
              "      <td>0.051624</td>\n",
              "      <td>0.027387</td>\n",
              "      <td>-0.855791</td>\n",
              "      <td>0.214892</td>\n",
              "      <td>0.225412</td>\n",
              "      <td>-0.133692</td>\n",
              "      <td>0.089966</td>\n",
              "      <td>0.106583</td>\n",
              "      <td>-0.387222</td>\n",
              "      <td>-0.086797</td>\n",
              "      <td>0.137901</td>\n",
              "      <td>0.026171</td>\n",
              "      <td>0.051993</td>\n",
              "      <td>-0.123805</td>\n",
              "      <td>0.036047</td>\n",
              "      <td>-0.025291</td>\n",
              "      <td>-0.037893</td>\n",
              "      <td>0.058400</td>\n",
              "      <td>0.116830</td>\n",
              "      <td>-0.212796</td>\n",
              "      <td>0.071412</td>\n",
              "      <td>0.069366</td>\n",
              "      <td>-0.057904</td>\n",
              "      <td>-0.056871</td>\n",
              "      <td>0.059438</td>\n",
              "      <td>-0.003105</td>\n",
              "      <td>0.056419</td>\n",
              "      <td>-0.334797</td>\n",
              "      <td>-0.253812</td>\n",
              "      <td>0.194060</td>\n",
              "      <td>-0.063695</td>\n",
              "      <td>-0.091296</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092233</td>\n",
              "      <td>0.053111</td>\n",
              "      <td>-0.143179</td>\n",
              "      <td>-0.267259</td>\n",
              "      <td>0.228425</td>\n",
              "      <td>0.055277</td>\n",
              "      <td>-0.091046</td>\n",
              "      <td>0.412173</td>\n",
              "      <td>0.275260</td>\n",
              "      <td>-0.120145</td>\n",
              "      <td>0.135697</td>\n",
              "      <td>0.072542</td>\n",
              "      <td>-0.007798</td>\n",
              "      <td>0.031437</td>\n",
              "      <td>-0.163737</td>\n",
              "      <td>0.053868</td>\n",
              "      <td>-0.996072</td>\n",
              "      <td>0.098439</td>\n",
              "      <td>0.266717</td>\n",
              "      <td>-0.080840</td>\n",
              "      <td>0.048672</td>\n",
              "      <td>-0.207145</td>\n",
              "      <td>-0.151934</td>\n",
              "      <td>0.121626</td>\n",
              "      <td>0.152315</td>\n",
              "      <td>-0.084608</td>\n",
              "      <td>-0.249602</td>\n",
              "      <td>0.197363</td>\n",
              "      <td>0.089792</td>\n",
              "      <td>-0.349206</td>\n",
              "      <td>0.119074</td>\n",
              "      <td>-0.175983</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>-0.093667</td>\n",
              "      <td>0.055091</td>\n",
              "      <td>0.035091</td>\n",
              "      <td>0.153164</td>\n",
              "      <td>-0.212592</td>\n",
              "      <td>-0.143454</td>\n",
              "      <td>0.035118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.283680</td>\n",
              "      <td>0.069567</td>\n",
              "      <td>0.101358</td>\n",
              "      <td>-0.129845</td>\n",
              "      <td>-0.097649</td>\n",
              "      <td>0.237560</td>\n",
              "      <td>-0.163427</td>\n",
              "      <td>-0.033652</td>\n",
              "      <td>0.069166</td>\n",
              "      <td>-0.982558</td>\n",
              "      <td>-0.014928</td>\n",
              "      <td>0.048511</td>\n",
              "      <td>-0.036828</td>\n",
              "      <td>0.150815</td>\n",
              "      <td>-0.104198</td>\n",
              "      <td>-0.241934</td>\n",
              "      <td>-0.051909</td>\n",
              "      <td>0.050744</td>\n",
              "      <td>-0.004248</td>\n",
              "      <td>-0.002488</td>\n",
              "      <td>-0.010589</td>\n",
              "      <td>0.153560</td>\n",
              "      <td>-0.018776</td>\n",
              "      <td>0.125332</td>\n",
              "      <td>-0.121310</td>\n",
              "      <td>0.100929</td>\n",
              "      <td>-0.213390</td>\n",
              "      <td>0.133119</td>\n",
              "      <td>0.142847</td>\n",
              "      <td>-0.132614</td>\n",
              "      <td>0.032284</td>\n",
              "      <td>0.419748</td>\n",
              "      <td>-0.364058</td>\n",
              "      <td>0.036675</td>\n",
              "      <td>-0.485144</td>\n",
              "      <td>0.312245</td>\n",
              "      <td>0.289430</td>\n",
              "      <td>-0.218514</td>\n",
              "      <td>-0.287215</td>\n",
              "      <td>...</td>\n",
              "      <td>0.256472</td>\n",
              "      <td>-0.254538</td>\n",
              "      <td>0.040987</td>\n",
              "      <td>-0.241147</td>\n",
              "      <td>0.395293</td>\n",
              "      <td>-0.409850</td>\n",
              "      <td>-0.192197</td>\n",
              "      <td>-0.122905</td>\n",
              "      <td>0.083810</td>\n",
              "      <td>0.118753</td>\n",
              "      <td>-0.162657</td>\n",
              "      <td>0.017149</td>\n",
              "      <td>0.010836</td>\n",
              "      <td>-0.022638</td>\n",
              "      <td>-0.160514</td>\n",
              "      <td>-0.050203</td>\n",
              "      <td>-0.885951</td>\n",
              "      <td>0.269781</td>\n",
              "      <td>0.176020</td>\n",
              "      <td>-0.140113</td>\n",
              "      <td>0.252006</td>\n",
              "      <td>-0.124844</td>\n",
              "      <td>0.063604</td>\n",
              "      <td>0.081165</td>\n",
              "      <td>-0.070500</td>\n",
              "      <td>-0.199715</td>\n",
              "      <td>0.240570</td>\n",
              "      <td>0.109020</td>\n",
              "      <td>-0.199160</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.025560</td>\n",
              "      <td>-0.143039</td>\n",
              "      <td>0.029881</td>\n",
              "      <td>-0.037473</td>\n",
              "      <td>-0.030373</td>\n",
              "      <td>0.087959</td>\n",
              "      <td>-0.280426</td>\n",
              "      <td>-0.162550</td>\n",
              "      <td>0.312706</td>\n",
              "      <td>-0.103671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>0</td>\n",
              "      <td>0.137752</td>\n",
              "      <td>0.237628</td>\n",
              "      <td>-0.037164</td>\n",
              "      <td>0.046723</td>\n",
              "      <td>-0.060013</td>\n",
              "      <td>0.021003</td>\n",
              "      <td>0.215228</td>\n",
              "      <td>-0.009179</td>\n",
              "      <td>0.256806</td>\n",
              "      <td>-1.261766</td>\n",
              "      <td>0.054473</td>\n",
              "      <td>0.051986</td>\n",
              "      <td>-0.084756</td>\n",
              "      <td>-0.015450</td>\n",
              "      <td>-0.088518</td>\n",
              "      <td>-0.046695</td>\n",
              "      <td>-0.068829</td>\n",
              "      <td>-0.029019</td>\n",
              "      <td>-0.051263</td>\n",
              "      <td>-0.153330</td>\n",
              "      <td>-0.142435</td>\n",
              "      <td>-0.320953</td>\n",
              "      <td>0.089712</td>\n",
              "      <td>-0.242919</td>\n",
              "      <td>0.207093</td>\n",
              "      <td>0.107357</td>\n",
              "      <td>0.066808</td>\n",
              "      <td>0.051429</td>\n",
              "      <td>0.036640</td>\n",
              "      <td>0.069964</td>\n",
              "      <td>-0.165560</td>\n",
              "      <td>0.091963</td>\n",
              "      <td>-0.059249</td>\n",
              "      <td>-0.372184</td>\n",
              "      <td>-0.550970</td>\n",
              "      <td>0.112322</td>\n",
              "      <td>-0.101686</td>\n",
              "      <td>-0.090855</td>\n",
              "      <td>-0.215788</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146830</td>\n",
              "      <td>-0.254172</td>\n",
              "      <td>0.039943</td>\n",
              "      <td>-0.084471</td>\n",
              "      <td>-0.131744</td>\n",
              "      <td>-0.054305</td>\n",
              "      <td>0.065564</td>\n",
              "      <td>0.173493</td>\n",
              "      <td>0.320623</td>\n",
              "      <td>0.185092</td>\n",
              "      <td>0.132057</td>\n",
              "      <td>-0.235625</td>\n",
              "      <td>0.194712</td>\n",
              "      <td>0.101396</td>\n",
              "      <td>-0.128172</td>\n",
              "      <td>0.137861</td>\n",
              "      <td>-0.832018</td>\n",
              "      <td>-0.093853</td>\n",
              "      <td>0.019242</td>\n",
              "      <td>0.027716</td>\n",
              "      <td>-0.324013</td>\n",
              "      <td>-0.037474</td>\n",
              "      <td>-0.286500</td>\n",
              "      <td>0.269803</td>\n",
              "      <td>0.064118</td>\n",
              "      <td>0.301843</td>\n",
              "      <td>0.068311</td>\n",
              "      <td>-0.252856</td>\n",
              "      <td>0.001829</td>\n",
              "      <td>0.008591</td>\n",
              "      <td>0.014037</td>\n",
              "      <td>-0.100103</td>\n",
              "      <td>0.114656</td>\n",
              "      <td>0.274558</td>\n",
              "      <td>0.229818</td>\n",
              "      <td>-0.022098</td>\n",
              "      <td>0.299954</td>\n",
              "      <td>-0.542680</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>-0.138706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.145344</td>\n",
              "      <td>0.082166</td>\n",
              "      <td>-0.029076</td>\n",
              "      <td>0.045717</td>\n",
              "      <td>-0.231986</td>\n",
              "      <td>-0.005544</td>\n",
              "      <td>0.193455</td>\n",
              "      <td>0.195786</td>\n",
              "      <td>-0.121839</td>\n",
              "      <td>-0.815428</td>\n",
              "      <td>-0.183185</td>\n",
              "      <td>-0.114376</td>\n",
              "      <td>-0.070910</td>\n",
              "      <td>-0.212040</td>\n",
              "      <td>-0.192466</td>\n",
              "      <td>-0.269231</td>\n",
              "      <td>-0.196218</td>\n",
              "      <td>-0.342628</td>\n",
              "      <td>0.124924</td>\n",
              "      <td>0.068990</td>\n",
              "      <td>0.075584</td>\n",
              "      <td>-0.188812</td>\n",
              "      <td>0.347516</td>\n",
              "      <td>0.090514</td>\n",
              "      <td>-0.264794</td>\n",
              "      <td>0.322066</td>\n",
              "      <td>-0.064849</td>\n",
              "      <td>0.193018</td>\n",
              "      <td>-0.052668</td>\n",
              "      <td>0.313483</td>\n",
              "      <td>-0.189308</td>\n",
              "      <td>0.312458</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>0.064308</td>\n",
              "      <td>-0.233966</td>\n",
              "      <td>0.143862</td>\n",
              "      <td>0.030297</td>\n",
              "      <td>-0.460872</td>\n",
              "      <td>-0.244145</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044242</td>\n",
              "      <td>-0.111320</td>\n",
              "      <td>0.089657</td>\n",
              "      <td>0.068977</td>\n",
              "      <td>-0.064313</td>\n",
              "      <td>-0.013455</td>\n",
              "      <td>-0.010585</td>\n",
              "      <td>0.099769</td>\n",
              "      <td>-0.006070</td>\n",
              "      <td>-0.274915</td>\n",
              "      <td>0.167041</td>\n",
              "      <td>-0.030705</td>\n",
              "      <td>0.222705</td>\n",
              "      <td>0.099264</td>\n",
              "      <td>-0.223865</td>\n",
              "      <td>-0.137089</td>\n",
              "      <td>-1.241644</td>\n",
              "      <td>0.177411</td>\n",
              "      <td>0.385716</td>\n",
              "      <td>0.251284</td>\n",
              "      <td>0.109574</td>\n",
              "      <td>-0.074128</td>\n",
              "      <td>-0.007959</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.007708</td>\n",
              "      <td>-0.246695</td>\n",
              "      <td>-0.144049</td>\n",
              "      <td>0.081175</td>\n",
              "      <td>-0.226814</td>\n",
              "      <td>-0.156460</td>\n",
              "      <td>0.164609</td>\n",
              "      <td>-0.164424</td>\n",
              "      <td>-0.165736</td>\n",
              "      <td>0.246015</td>\n",
              "      <td>0.171636</td>\n",
              "      <td>0.092415</td>\n",
              "      <td>-0.132593</td>\n",
              "      <td>-0.020160</td>\n",
              "      <td>-0.032738</td>\n",
              "      <td>-0.141213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.299538</td>\n",
              "      <td>0.092976</td>\n",
              "      <td>0.106222</td>\n",
              "      <td>-0.187738</td>\n",
              "      <td>-0.001007</td>\n",
              "      <td>-0.014380</td>\n",
              "      <td>-0.208287</td>\n",
              "      <td>0.010788</td>\n",
              "      <td>0.110072</td>\n",
              "      <td>-1.308856</td>\n",
              "      <td>0.006228</td>\n",
              "      <td>0.155496</td>\n",
              "      <td>0.099507</td>\n",
              "      <td>-0.003538</td>\n",
              "      <td>-0.053377</td>\n",
              "      <td>-0.145253</td>\n",
              "      <td>-0.215576</td>\n",
              "      <td>-0.153290</td>\n",
              "      <td>-0.039816</td>\n",
              "      <td>-0.030433</td>\n",
              "      <td>-0.033172</td>\n",
              "      <td>0.052822</td>\n",
              "      <td>0.069506</td>\n",
              "      <td>-0.005236</td>\n",
              "      <td>-0.045356</td>\n",
              "      <td>-0.015904</td>\n",
              "      <td>-0.110563</td>\n",
              "      <td>0.039176</td>\n",
              "      <td>-0.055899</td>\n",
              "      <td>0.103209</td>\n",
              "      <td>0.079852</td>\n",
              "      <td>0.250597</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>0.232991</td>\n",
              "      <td>-0.374249</td>\n",
              "      <td>0.055424</td>\n",
              "      <td>0.040693</td>\n",
              "      <td>-0.142387</td>\n",
              "      <td>-0.149773</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.140979</td>\n",
              "      <td>0.198289</td>\n",
              "      <td>-0.254772</td>\n",
              "      <td>-0.544894</td>\n",
              "      <td>-0.025016</td>\n",
              "      <td>0.289078</td>\n",
              "      <td>0.142474</td>\n",
              "      <td>0.086486</td>\n",
              "      <td>0.091514</td>\n",
              "      <td>-0.061899</td>\n",
              "      <td>-0.048974</td>\n",
              "      <td>-0.151630</td>\n",
              "      <td>-0.086963</td>\n",
              "      <td>-0.310516</td>\n",
              "      <td>-0.411893</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>-0.886036</td>\n",
              "      <td>-0.015132</td>\n",
              "      <td>0.162988</td>\n",
              "      <td>0.076660</td>\n",
              "      <td>0.042451</td>\n",
              "      <td>-0.411903</td>\n",
              "      <td>-0.159362</td>\n",
              "      <td>0.144227</td>\n",
              "      <td>-0.114352</td>\n",
              "      <td>-0.102048</td>\n",
              "      <td>-0.083292</td>\n",
              "      <td>-0.214574</td>\n",
              "      <td>-0.062870</td>\n",
              "      <td>-0.282682</td>\n",
              "      <td>0.223478</td>\n",
              "      <td>-0.080404</td>\n",
              "      <td>-0.182246</td>\n",
              "      <td>-0.062546</td>\n",
              "      <td>0.197915</td>\n",
              "      <td>0.071920</td>\n",
              "      <td>0.199088</td>\n",
              "      <td>-0.129117</td>\n",
              "      <td>-0.050573</td>\n",
              "      <td>-0.222812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.251889</td>\n",
              "      <td>-0.012373</td>\n",
              "      <td>0.014143</td>\n",
              "      <td>-0.042136</td>\n",
              "      <td>-0.240158</td>\n",
              "      <td>-0.033394</td>\n",
              "      <td>0.081844</td>\n",
              "      <td>0.057246</td>\n",
              "      <td>-0.004319</td>\n",
              "      <td>-1.091398</td>\n",
              "      <td>-0.077288</td>\n",
              "      <td>0.129734</td>\n",
              "      <td>-0.021128</td>\n",
              "      <td>-0.127366</td>\n",
              "      <td>-0.180829</td>\n",
              "      <td>-0.007107</td>\n",
              "      <td>0.132801</td>\n",
              "      <td>-0.087933</td>\n",
              "      <td>-0.009424</td>\n",
              "      <td>-0.045595</td>\n",
              "      <td>0.034451</td>\n",
              "      <td>-0.042688</td>\n",
              "      <td>-0.016915</td>\n",
              "      <td>-0.083584</td>\n",
              "      <td>-0.112574</td>\n",
              "      <td>0.078643</td>\n",
              "      <td>-0.024629</td>\n",
              "      <td>0.084536</td>\n",
              "      <td>0.031689</td>\n",
              "      <td>0.091825</td>\n",
              "      <td>-0.099220</td>\n",
              "      <td>0.210011</td>\n",
              "      <td>0.078526</td>\n",
              "      <td>0.015354</td>\n",
              "      <td>-0.330495</td>\n",
              "      <td>0.012358</td>\n",
              "      <td>-0.054802</td>\n",
              "      <td>-0.050272</td>\n",
              "      <td>-0.072328</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100902</td>\n",
              "      <td>-0.344626</td>\n",
              "      <td>0.068841</td>\n",
              "      <td>0.062270</td>\n",
              "      <td>0.193045</td>\n",
              "      <td>-0.247352</td>\n",
              "      <td>0.041152</td>\n",
              "      <td>0.046642</td>\n",
              "      <td>0.004974</td>\n",
              "      <td>0.094002</td>\n",
              "      <td>0.020330</td>\n",
              "      <td>0.183988</td>\n",
              "      <td>0.210426</td>\n",
              "      <td>0.042643</td>\n",
              "      <td>0.073365</td>\n",
              "      <td>0.135687</td>\n",
              "      <td>-1.404390</td>\n",
              "      <td>-0.092540</td>\n",
              "      <td>0.096655</td>\n",
              "      <td>0.019697</td>\n",
              "      <td>-0.048327</td>\n",
              "      <td>-0.000718</td>\n",
              "      <td>-0.083539</td>\n",
              "      <td>0.045086</td>\n",
              "      <td>-0.161181</td>\n",
              "      <td>0.160841</td>\n",
              "      <td>-0.134312</td>\n",
              "      <td>0.107327</td>\n",
              "      <td>0.059690</td>\n",
              "      <td>0.093189</td>\n",
              "      <td>-0.063868</td>\n",
              "      <td>-0.023310</td>\n",
              "      <td>-0.056357</td>\n",
              "      <td>-0.210712</td>\n",
              "      <td>0.113685</td>\n",
              "      <td>-0.071614</td>\n",
              "      <td>-0.318756</td>\n",
              "      <td>-0.372560</td>\n",
              "      <td>0.214112</td>\n",
              "      <td>-0.048750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 3901 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Y    Body_1    Body_2  ...  Title_298  Title_299  Title_300\n",
              "0      1 -0.340877  0.215006  ...  -0.090500   0.019662   0.019357\n",
              "1      1 -0.317487  0.283310  ...  -0.289137  -0.033799  -0.084037\n",
              "2      2 -0.219626  0.050607  ...   0.051204   0.791200  -0.713580\n",
              "3      1 -0.274443  0.032652  ...   0.190240   0.212298   0.092343\n",
              "4      1 -0.174143  0.054057  ...  -0.212592  -0.143454   0.035118\n",
              "...   ..       ...       ...  ...        ...        ...        ...\n",
              "14995  0 -0.283680  0.069567  ...  -0.162550   0.312706  -0.103671\n",
              "14996  0  0.137752  0.237628  ...  -0.542680   0.025890  -0.138706\n",
              "14997  1 -0.145344  0.082166  ...  -0.020160  -0.032738  -0.141213\n",
              "14998  0 -0.299538  0.092976  ...  -0.129117  -0.050573  -0.222812\n",
              "14999  0 -0.251889 -0.012373  ...  -0.372560   0.214112  -0.048750\n",
              "\n",
              "[15000 rows x 3901 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rizee5Ae1ata"
      },
      "source": [
        "Principal component analysis  for dimensionality reduction to reduce the number of features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6wR-qgijHm8"
      },
      "source": [
        "pca5 = PCA(n_components=800)\n",
        "pca5.fit(X)\n",
        "X_trans2000_train = pca5.transform(X)\n",
        "X_trans2000_test = pca5.transform(validX)\n",
        "exp_vars = pca5.explained_variance_ratio_\n",
        "#plt.plot(range(1,X.shape[1]+1),exp_vars,'--o')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nNhzv4h2OLu"
      },
      "source": [
        "Plot the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "Su2oKY867L4P",
        "outputId": "558b1d97-89c3-44b7-8137-da8239f418a0"
      },
      "source": [
        "cum_sum = np.cumsum(exp_vars)\n",
        "#plt.figure(figsize(10,10))\n",
        "plt.plot(range(len(exp_vars)),cum_sum,'-o')\n",
        "plt.xlabel('features', fontsize=15)\n",
        "plt.ylabel('cumulative variance', fontsize=15)\n",
        "plt.title('explained variance v/s features', fontsize=18)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'explained variance v/s features')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miN9QhlVZMnr"
      },
      "source": [
        "train_PCA = pd.DataFrame(data=X_trans2000_train)\n",
        "train_PCA.to_csv('/content/drive/MyDrive/EE769Project/TrainingPCA2000.csv')\n",
        "train_PCA = pd.DataFrame(data=X_trans2000_test)\n",
        "train_PCA.to_csv('/content/drive/MyDrive/EE769Project/TestingPCA2000.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg1RcNFIg_8L"
      },
      "source": [
        "exp_var = pd.DataFrame(data=pca5.explained_variance_ratio_)\n",
        "exp_var.to_csv('/content/drive/MyDrive/EE769Project/TestingPCA2000.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUaK0Bo0IuF0"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score # ROC and F1 are balanced metrics"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3drm8wJZQ8"
      },
      "source": [
        "Training **Random Forest Classifier** and tuning it for different hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EqOnpFHPD3SX",
        "outputId": "ce4301bd-aa52-4b0c-fd2d-127d10237e3d"
      },
      "source": [
        "rfc = RandomForestClassifier()\n",
        "\n",
        "scoring = 'f1_weighted'\n",
        "\n",
        "print('Training RFC using GridSearchCV')\n",
        "\n",
        "hyperparameters = {'max_depth': [10,20,25], 'n_estimators':[50,100,150,200]}\n",
        "\n",
        "clf = GridSearchCV(rfc, cv=5, param_grid=hyperparameters,  scoring=scoring, verbose=2.1)\n",
        "clf.fit(np.array(X_trans2000_train), np.squeeze(y))\n",
        "print('Best parameters:')\n",
        "print(clf.best_params_)\n",
        "print('Best' + scoring + \":\" + str(clf.best_score_))\n",
        "\n",
        "print('Train classification report:')\n",
        "y_true, y_pred = np.squeeze(y), clf.predict(np.array(X_trans2000_train))\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print('Test classification report:')\n",
        "y_true, y_pred = np.squeeze(validY), clf.predict(np.array(X_trans2000_test))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training RFC using GridSearchCV\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] max_depth=10, n_estimators=50 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=10, n_estimators=50, score=0.591, total= 1.3min\n",
            "[CV] max_depth=10, n_estimators=50 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=10, n_estimators=50, score=0.596, total= 1.3min\n",
            "[CV] max_depth=10, n_estimators=50 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=10, n_estimators=50, score=0.590, total= 1.3min\n",
            "[CV] max_depth=10, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=10, n_estimators=50, score=0.589, total= 1.3min\n",
            "[CV] max_depth=10, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=10, n_estimators=50, score=0.587, total= 1.3min\n",
            "[CV] max_depth=10, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=100, score=0.603, total= 2.6min\n",
            "[CV] max_depth=10, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=100, score=0.612, total= 2.6min\n",
            "[CV] max_depth=10, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=100, score=0.597, total= 2.6min\n",
            "[CV] max_depth=10, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=100, score=0.610, total= 2.6min\n",
            "[CV] max_depth=10, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=100, score=0.605, total= 2.6min\n",
            "[CV] max_depth=10, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=150, score=0.610, total= 3.9min\n",
            "[CV] max_depth=10, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=150, score=0.614, total= 3.9min\n",
            "[CV] max_depth=10, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=150, score=0.600, total= 3.9min\n",
            "[CV] max_depth=10, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=150, score=0.612, total= 3.9min\n",
            "[CV] max_depth=10, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=150, score=0.599, total= 3.9min\n",
            "[CV] max_depth=10, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=200, score=0.614, total= 5.2min\n",
            "[CV] max_depth=10, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=200, score=0.620, total= 5.3min\n",
            "[CV] max_depth=10, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=200, score=0.606, total= 5.2min\n",
            "[CV] max_depth=10, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=200, score=0.617, total= 5.3min\n",
            "[CV] max_depth=10, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=10, n_estimators=200, score=0.610, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=20, n_estimators=50, score=0.557, total= 1.6min\n",
            "[CV] max_depth=20, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=20, n_estimators=50, score=0.551, total= 1.6min\n",
            "[CV] max_depth=20, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=20, n_estimators=50, score=0.550, total= 1.7min\n",
            "[CV] max_depth=20, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=20, n_estimators=50, score=0.562, total= 1.6min\n",
            "[CV] max_depth=20, n_estimators=50 ...................................\n",
            "[CV] ....... max_depth=20, n_estimators=50, score=0.547, total= 1.6min\n",
            "[CV] max_depth=20, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=100, score=0.578, total= 3.3min\n",
            "[CV] max_depth=20, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=100, score=0.600, total= 3.3min\n",
            "[CV] max_depth=20, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=100, score=0.595, total= 3.2min\n",
            "[CV] max_depth=20, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=100, score=0.594, total= 3.2min\n",
            "[CV] max_depth=20, n_estimators=100 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=100, score=0.584, total= 3.3min\n",
            "[CV] max_depth=20, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=150, score=0.612, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=150, score=0.610, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=150, score=0.602, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=150, score=0.611, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=150 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=150, score=0.595, total= 4.9min\n",
            "[CV] max_depth=20, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=200, score=0.611, total= 6.6min\n",
            "[CV] max_depth=20, n_estimators=200 ..................................\n",
            "[CV] ...... max_depth=20, n_estimators=200, score=0.614, total= 6.5min\n",
            "[CV] max_depth=20, n_estimators=200 ..................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f27212ce0ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trans2000_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best parameters:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxXeUbzzJf8Q"
      },
      "source": [
        "The best hyperparameters after tuning comes out to be $max\\ depth\\ as\\ 20$ and $n\\ estimators\\ as\\ 200$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVg8nevG-57r",
        "outputId": "02f4a500-59d1-42de-d8f4-c8b9bb618426"
      },
      "source": [
        "rfc = RandomForestClassifier()\n",
        "\n",
        "scoring = 'f1_weighted'\n",
        "\n",
        "print('Training RFC using GridSearchCV')\n",
        "\n",
        "hyperparameters = {'max_depth': [5], 'n_estimators':[200]}\n",
        "\n",
        "clf = GridSearchCV(rfc, cv=5, param_grid=hyperparameters,  scoring=scoring, verbose=2.1)\n",
        "clf.fit(np.array(X_trans2000_train), np.squeeze(y))\n",
        "print('Best parameters:')\n",
        "print(clf.best_params_)\n",
        "print('Best' + scoring + \":\" + str(clf.best_score_))\n",
        "\n",
        "print('Train classification report:')\n",
        "y_true, y_pred = np.squeeze(y), clf.predict(np.array(X_trans2000_train))\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print('Test classification report:')\n",
        "y_true, y_pred = np.squeeze(validY), clf.predict(np.array(X_trans2000_test))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training RFC using GridSearchCV\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV] max_depth=5, n_estimators=200 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=5, n_estimators=200, score=0.579, total= 3.2min\n",
            "[CV] max_depth=5, n_estimators=200 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=5, n_estimators=200, score=0.588, total= 3.2min\n",
            "[CV] max_depth=5, n_estimators=200 ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... max_depth=5, n_estimators=200, score=0.569, total= 3.1min\n",
            "[CV] max_depth=5, n_estimators=200 ...................................\n",
            "[CV] ....... max_depth=5, n_estimators=200, score=0.577, total= 3.1min\n",
            "[CV] max_depth=5, n_estimators=200 ...................................\n",
            "[CV] ....... max_depth=5, n_estimators=200, score=0.575, total= 3.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters:\n",
            "{'max_depth': 5, 'n_estimators': 200}\n",
            "Bestf1_weighted:0.57750419448965\n",
            "Train classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.57      0.61     15000\n",
            "           1       0.66      0.46      0.54     15000\n",
            "           2       0.58      0.82      0.68     15000\n",
            "\n",
            "    accuracy                           0.62     45000\n",
            "   macro avg       0.63      0.62      0.61     45000\n",
            "weighted avg       0.63      0.62      0.61     45000\n",
            "\n",
            "Test classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.55      0.58      5000\n",
            "           1       0.63      0.41      0.50      5000\n",
            "           2       0.56      0.81      0.66      5000\n",
            "\n",
            "    accuracy                           0.59     15000\n",
            "   macro avg       0.60      0.59      0.58     15000\n",
            "weighted avg       0.60      0.59      0.58     15000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPQMF5e66Lf_"
      },
      "source": [
        "Training the **XG Boost Classfier** on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_X943GbTRPJ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcmTKslR6BSu"
      },
      "source": [
        "Mean square errors before and after splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXnL7sJKb_j3",
        "outputId": "6209673c-26ca-4446-a4d4-62b5d3165aed"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split as ttsplit\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "\n",
        "# split data into training and testing sets\n",
        "# then split training set in half\n",
        "#X_train, X_test, y_train, y_test = ttsplit(X, y, test_size=0.1, random_state=0)\n",
        "X_train_1, X_train_2, y_train_1, y_train_2 = ttsplit(X_trans2000_train,y,test_size=0.5,random_state=1)\n",
        "\n",
        "xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\n",
        "xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\n",
        "xg_test = xgb.DMatrix(X_trans2000_test, label=validY)\n",
        "\n",
        "params = {'objective': 'reg:squarederror', 'verbose': False}\n",
        "model_1 = xgb.train(params, xg_train_1, 30)\n",
        "model_1.save_model('model_1.model')\n",
        "\n",
        "# ================= train two versions of the model =====================#\n",
        "model_2_v1 = xgb.train(params, xg_train_2, 30)\n",
        "model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n",
        "\n",
        "print(mse(model_1.predict(xg_test), validY))     # benchmark\n",
        "print(mse(model_2_v1.predict(xg_test), validY))  # \"before\"\n",
        "print(mse(model_2_v2.predict(xg_test), validY))  # \"after\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5122351461524909\n",
            "0.5058494469864965\n",
            "0.5053648113670353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh4jntuF66q0",
        "outputId": "554881f4-ce61-40e1-cb81-51cab66c07c9"
      },
      "source": [
        "print(model_1.predict(xg_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.1222706  0.9030149  1.2960899  ... 1.5764829  0.35355026 1.7275496 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UCrvIqRZV0w6",
        "outputId": "d301b104-5a76-41ec-c724-d12722ea5684"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "model = XGBClassifier(learning_rate = 0.05, n_estimators=700, max_depth=4)\n",
        "model.fit(X_trans2000_train, np.squeeze(y))\n",
        "\n",
        "# make predictions for test set\n",
        "#y_pred = model.predict(np.array(X_trans2000_test))\n",
        "#predictions = [round(value) for value in y_pred]\n",
        "\n",
        "print('Train classification report:')\n",
        "y_true, y_pred = np.squeeze(y), model.predict(np.array(X_trans2000_train))\n",
        "predictions = [round(value) for value in y_pred]\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print('Test classification report:')\n",
        "y_true1, y_pred1 = np.squeeze(validY), model.predict(np.array(X_trans2000_test))\n",
        "predictions1 = [round(value) for value in y_pred1]\n",
        "print(classification_report(y_true1, y_pred1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.82      0.82     15000\n",
            "           1       0.84      0.77      0.81     15000\n",
            "           2       0.84      0.90      0.87     15000\n",
            "\n",
            "    accuracy                           0.83     45000\n",
            "   macro avg       0.83      0.83      0.83     45000\n",
            "weighted avg       0.83      0.83      0.83     45000\n",
            "\n",
            "Test classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.63      0.63      5000\n",
            "           1       0.68      0.61      0.64      5000\n",
            "           2       0.70      0.79      0.74      5000\n",
            "\n",
            "    accuracy                           0.68     15000\n",
            "   macro avg       0.67      0.68      0.67     15000\n",
            "weighted avg       0.67      0.68      0.67     15000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Uxsd7_QPpa",
        "outputId": "7e8b833c-0a39-410d-e83a-48a766e5994a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(np.squeeze(validY), predictions1)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 67.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACWh8ZggI_vD"
      },
      "source": [
        "Training **Neural network** by using ReLU and sigmoid activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLqrOhnnynKs",
        "outputId": "17c1ca3b-b8a2-4056-f456-225ea0a04d14"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=800, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(np.array(X_trans2000_train), np.squeeze(y), validation_data=(X_trans2000_test,validY), epochs=10, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "900/900 [==============================] - 3s 2ms/step - loss: 0.6585 - accuracy: 0.3482 - val_loss: 0.5514 - val_accuracy: 0.4318\n",
            "Epoch 2/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.4443 - val_loss: 0.5361 - val_accuracy: 0.4539\n",
            "Epoch 3/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.5053 - accuracy: 0.4758 - val_loss: 0.5263 - val_accuracy: 0.4551\n",
            "Epoch 4/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4905 - accuracy: 0.4908 - val_loss: 0.5204 - val_accuracy: 0.4681\n",
            "Epoch 5/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4805 - accuracy: 0.4996 - val_loss: 0.5203 - val_accuracy: 0.4787\n",
            "Epoch 6/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4705 - accuracy: 0.5134 - val_loss: 0.5129 - val_accuracy: 0.4834\n",
            "Epoch 7/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4591 - accuracy: 0.5270 - val_loss: 0.5078 - val_accuracy: 0.4892\n",
            "Epoch 8/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4547 - accuracy: 0.5304 - val_loss: 0.5071 - val_accuracy: 0.4878\n",
            "Epoch 9/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4498 - accuracy: 0.5362 - val_loss: 0.5086 - val_accuracy: 0.4883\n",
            "Epoch 10/10\n",
            "900/900 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.5431 - val_loss: 0.5081 - val_accuracy: 0.4935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0627f9a3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gSdeJbtdM1YP",
        "outputId": "0783450f-c227-467b-b00f-08a8a9a09ab6"
      },
      "source": [
        "from xgboost import XGBClassifier, plot_importance\n",
        "plt.figure(figsize=(50,50))\n",
        "fit = XGBClassifier().fit(np.array(X_trans2000_train),np.squeeze(y))\n",
        "plot_importance(fit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f883ada6490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}